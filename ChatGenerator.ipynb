{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f558eb78-7683-46d5-ad22-38faa21d41c0",
      "metadata": {
        "id": "f558eb78-7683-46d5-ad22-38faa21d41c0"
      },
      "outputs": [],
      "source": [
        "# RNN which will (hopefully) generate a random chat between 2 people"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "cdedd7d8-34c6-4e67-b723-2b3401656923",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdedd7d8-34c6-4e67-b723-2b3401656923",
        "outputId": "3596cd22-ca96-4a0c-c0cf-81476f49f9d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "# # for kaggle\n",
        "# # import kaggle\n",
        "# for work\n",
        "import torch\n",
        "import nltk # to tokenize the text\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "c1a3f73c-0563-4923-9a57-1e2ee508425c",
      "metadata": {
        "id": "c1a3f73c-0563-4923-9a57-1e2ee508425c"
      },
      "outputs": [],
      "source": [
        "# get and unzip dataset - only need to run once\n",
        "# dataset = 'projjal1/human-conversation-training-data'\n",
        "# kaggle.api.dataset_download_files(dataset, path=\".\", unzip=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "b8a1a826-fdde-42ae-84b3-71230c1105a0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8a1a826-fdde-42ae-84b3-71230c1105a0",
        "outputId": "b53e0581-5e28-4e32-bbb3-668d2e798bb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total characters in text:  115782\n",
            "Human 1: Hi!\n",
            "Human 2: What is your favorite holiday?\n",
            "Human 1: one where I get to meet lots of different people.\n",
            "Human 2: What was the most number of people you have ever met during a holiday?\n",
            "Human 1: Hard to keep a count. Maybe 25.\n",
            "Human 2: Which holiday was that?\n",
            "Human 1: I think it was Australia\n",
            "Human 2: Do you still talk to the people you met?\n",
            "Human 1: Not really. The interactions are usually short-lived but it's fascinating to learn where people are coming from and what matters to them\n"
          ]
        }
      ],
      "source": [
        "# reading the data line by line into list\n",
        "text = None\n",
        "with open('human_chat.txt','r',encoding='utf-8') as dataset_file:\n",
        "    text = dataset_file.read()\n",
        "print(\"Total characters in text: \",len(text))\n",
        "print(text[:495])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "62f16d83-ef5d-4a6a-8fa8-6bcb7a5c68c5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62f16d83-ef5d-4a6a-8fa8-6bcb7a5c68c5",
        "outputId": "37245f1a-b2d0-495c-ded2-c0cf53cb2e42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['human', '1', ':', 'hi', '!', 'human', '2', ':', 'what', 'is', 'your', 'favorite', 'holiday', '?', 'human', '1', ':', 'one', 'where', 'i', 'get', 'to', 'meet', 'lots', 'of', 'different', 'people', '.', 'human', '2', ':', 'what', 'was', 'the', 'most', 'number', 'of', 'people', 'you', 'have', 'ever', 'met', 'during', 'a', 'holiday', '?', 'human', '1', ':', 'hard', 'to', 'keep', 'a', 'count', '.', 'maybe', '25.', 'human', '2', ':', 'which', 'holiday', 'was', 'that', '?', 'human', '1', ':', 'i', 'think', 'it', 'was', 'australia', 'human', '2', ':', 'do', 'you', 'still', 'talk', 'to', 'the', 'people', 'you', 'met', '?', 'human', '1', ':', 'not', 'really', '.', 'the', 'interactions', 'are', 'usually', 'short-lived', 'but', 'it', \"'s\"]\n",
            "Total words:  27943\n",
            "Unique words:  2813\n"
          ]
        }
      ],
      "source": [
        "# building vocabulary in token form\n",
        "# words = [line.lower().split() for line in lines] # using nltk tokens for better splitting (fine! vs fine)\n",
        "words = word_tokenize(text.lower()) # get tokens in the form of words and punctuations from the text\n",
        "print(words[:100])\n",
        "print(\"Total words: \",len(words))\n",
        "print(\"Unique words: \", len(set((words))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "535fbfdd-a016-4182-afae-6f05340f05a7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "535fbfdd-a016-4182-afae-6f05340f05a7",
        "outputId": "53df05ed-80a9-4252-c598-41b2c2ea25a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2813\n",
            "['!', '%', '&', \"'\", \"''\", \"'billions\", \"'court\", \"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'ve\", '(', ')', '*', '+', ',', '-', '--', '.', '..', '...', '....', '/', '1', '1-2', '1.', '10', '114', '12', '15', '1:1', '2', '2-3', '2.', '20', '2019', '23rd', '24', '24th', '25.', '3', '3.', '30c', '320', '3pm', '4', '4.', '4.30']\n"
          ]
        }
      ],
      "source": [
        "vocabulary = sorted(set(words)) # sorts the words alphabetically\n",
        "print(len(vocabulary))\n",
        "print(vocabulary[:50])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "61436d04-433a-42d9-a5e5-4a10e65e9482",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61436d04-433a-42d9-a5e5-4a10e65e9482",
        "outputId": "d2a8fee9-9e9a-4149-bb02-6605dfc755d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['!' '%' '&' \"'\" \"''\" \"'billions\" \"'court\" \"'d\" \"'ll\" \"'m\" \"'re\" \"'s\"\n",
            " \"'ve\" '(' ')' '*' '+' ',' '-' '--' '.' '..' '...' '....' '/' '1' '1-2'\n",
            " '1.' '10' '114' '12' '15' '1:1' '2' '2-3' '2.' '20' '2019' '23rd' '24'\n",
            " '24th' '25.' '3' '3.' '30c' '320' '3pm' '4' '4.' '4.30']\n"
          ]
        }
      ],
      "source": [
        "# create lookup table for word to in and reverse\n",
        "word2int_mapping = {word:i for i,word in enumerate(vocabulary)}\n",
        "word_array = np.array(vocabulary) # reverse of word2int_mapping, the index holds the word\n",
        "# print(word2int_mapping)\n",
        "print(word_array[:50])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "f5880daa-4023-4209-a48a-0064f440e256",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5880daa-4023-4209-a48a-0064f440e256",
        "outputId": "4017f3d7-42bd-41e2-c33e-9dfb16ab9ec3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1187   25   56 ... 1574 1281   20]\n",
            "Encoded lines shape: (27943,)\n",
            "Words ['human', '1', ':', 'hi', '!', 'human', '2', ':', 'what', 'is', 'your', 'favorite', 'holiday', '?', 'human', '1', ':', 'one', 'where', 'i']\n",
            "Encoding, [1187   25   56 1134    0 1187   33   56 2708 1278 2783  876 1159   60\n",
            " 1187   25   56 1664 2713 1203]\n",
            "Reverse conversion:  human 1 : hi ! human 2 : what is your favorite holiday ? human 1 : one where i\n"
          ]
        }
      ],
      "source": [
        "# encoding and decoding the sentences according to the vocabulary\n",
        "encoded_lines = np.array([word2int_mapping[word] for word in words], dtype=np.int32)\n",
        "print(encoded_lines)\n",
        "print(\"Encoded lines shape:\",encoded_lines.shape)\n",
        "print(\"Words\", words[:20])\n",
        "print(\"Encoding,\", encoded_lines[:20])\n",
        "print(\"Reverse conversion: \", ' '.join(word_array[encoded_lines[:20]])) # ' '.join(word_array[np.array(list_of_indices))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "50c29fb7-cdc0-4cb8-8615-db08c8667aa2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50c29fb7-cdc0-4cb8-8615-db08c8667aa2",
        "outputId": "2c8d616e-2286-4409-84f5-0cbeaa630ca6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([1187,   25,   56, 1134,    0, 1187,   33,   56, 2708, 1278, 2783,\n",
            "        876, 1159,   60, 1187,   25,   56, 1664, 2713, 1203, 1009, 2501,\n",
            "       1491, 1427, 1642,  688, 1762,   20, 1187,   33,   56, 2708, 2681,\n",
            "       2461, 1556, 1628, 1642, 1762, 2781, 1103,  812, 1512,  749,   63,\n",
            "       1159,   60, 1187,   25,   56, 1095, 2501], dtype=int32)]\n",
            "\n",
            "[1187   25   56 1134    0 1187   33   56 2708 1278 2783  876 1159   60\n",
            " 1187   25   56 1664 2713 1203 1009 2501 1491 1427 1642  688 1762   20\n",
            " 1187   33   56 2708 2681 2461 1556 1628 1642 1762 2781 1103  812 1512\n",
            "  749   63 1159   60 1187   25   56 1095] -> 2501\n",
            "'human 1 : hi ! human 2 : what is your favorite holiday ? human 1 : one where i get to meet lots of different people . human 2 : what was the most number of people you have ever met during a holiday ? human 1 : hard' -> 'to'\n"
          ]
        }
      ],
      "source": [
        "# make sequences and chunks\n",
        "sequence_size = 50 # input sequence length\n",
        "chunk_size = sequence_size + 1\n",
        "\n",
        "text_chunks = [encoded_lines[i:i+chunk_size] for i in range(len(encoded_lines)-chunk_size+1)] # make text chunks of sequence size\n",
        "# each chunk start +1 to the right of the previous\n",
        "print(text_chunks[:1],end=\"\\n\\n\")\n",
        "\n",
        "for seq in text_chunks[:1]:\n",
        "    input_seq = seq[:sequence_size] # input sequence = 50 words\n",
        "    target = seq[sequence_size] # target output = one word\n",
        "    print(input_seq,'->',target)\n",
        "    print(repr(' '.join(word_array[input_seq])),'->',repr(''.join(word_array[target])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "62fb0a5e-d5b4-4b5b-835c-a33482f3ab6d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62fb0a5e-d5b4-4b5b-835c-a33482f3ab6d",
        "outputId": "141b1331-a819-421d-f23a-a9bbdc105fcd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-3c79b1f88e2a>:12: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:274.)\n",
            "  seq_dataset = TextDataset(torch.tensor((text_chunks)))\n"
          ]
        }
      ],
      "source": [
        "# convert into proper pytorch dataset form\n",
        "\n",
        "class TextDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, text_chunks):\n",
        "        self.text_chunks = text_chunks\n",
        "    def __len__(self):\n",
        "        return len(self.text_chunks) # number of text chunks\n",
        "    def __getitem__(self,idx):\n",
        "        text_chunk = self.text_chunks[idx] # return the idxth text chunk\n",
        "        return text_chunk[:-1].long(), text_chunk[1:].long() # return it in 2 parts: all words cept last (input sequence) and all words cept 1st (target sequence)\n",
        "\n",
        "seq_dataset = TextDataset(torch.tensor((text_chunks)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "aa8e8bed-1363-420f-b9a1-eb4a70dd13cb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aa8e8bed-1363-420f-b9a1-eb4a70dd13cb",
        "outputId": "97c7e48b-056b-4d80-891d-15ba4671e709"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: 'human 1 : hi ! human 2 : what is your favorite holiday ? human 1 : one where i get to meet lots of different people . human 2 : what was the most number of people you have ever met during a holiday ? human 1 : hard'\n",
            "Target: '1 : hi ! human 2 : what is your favorite holiday ? human 1 : one where i get to meet lots of different people . human 2 : what was the most number of people you have ever met during a holiday ? human 1 : hard to'\n",
            "\n",
            "Input: '1 : hi ! human 2 : what is your favorite holiday ? human 1 : one where i get to meet lots of different people . human 2 : what was the most number of people you have ever met during a holiday ? human 1 : hard to'\n",
            "Target: ': hi ! human 2 : what is your favorite holiday ? human 1 : one where i get to meet lots of different people . human 2 : what was the most number of people you have ever met during a holiday ? human 1 : hard to keep'\n",
            "\n",
            "Input: ': hi ! human 2 : what is your favorite holiday ? human 1 : one where i get to meet lots of different people . human 2 : what was the most number of people you have ever met during a holiday ? human 1 : hard to keep'\n",
            "Target: 'hi ! human 2 : what is your favorite holiday ? human 1 : one where i get to meet lots of different people . human 2 : what was the most number of people you have ever met during a holiday ? human 1 : hard to keep a'\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# checking out our get item chunks functionality\n",
        "for i,(seq,target) in enumerate(seq_dataset): # gets items in order using overloaded getitem\n",
        "    print(\"Input:\",repr(' '.join(word_array[seq])))\n",
        "    print(\"Target:\",repr(' '.join(word_array[target])))\n",
        "    print()\n",
        "    if i == 2:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "69a4d405-b970-46a0-8d43-cec620fb5986",
      "metadata": {
        "id": "69a4d405-b970-46a0-8d43-cec620fb5986"
      },
      "outputs": [],
      "source": [
        "# create a dataloader\n",
        "batch_size = 32\n",
        "dataloader = torch.utils.data.DataLoader(seq_dataset,batch_size=batch_size,shuffle=True,drop_last=True) # drop last batch if it is smaller than batch size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "3f9e71bd-6a34-4d30-809b-9cbe6bc62b51",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3f9e71bd-6a34-4d30-809b-9cbe6bc62b51",
        "outputId": "9cf4435b-3dae-4940-c4e5-895bb13cc9ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "# this code tells what to use, not neccessary if u dont have a gpu to run on\n",
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "34fc40fe-1b05-4960-8155-12e8271d42fa",
      "metadata": {
        "id": "34fc40fe-1b05-4960-8155-12e8271d42fa"
      },
      "outputs": [],
      "source": [
        "# my rnn\n",
        "class ChatGeneratingRNN(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.embedding = torch.nn.Embedding(vocab_size, embed_size) # embedds the encodings into vectors\n",
        "        self.rnn_hidden_size = hidden_size # num hidden layer neurons\n",
        "        self.rnn = torch.nn.LSTM(embed_size, hidden_size, batch_first=True) # LSTM layer as hidden layer\n",
        "        self.fc = torch.nn.Linear(hidden_size, vocab_size) # fully connected layer as output layer\n",
        "\n",
        "    def forward(self, inputs, hidden, cell):\n",
        "        layer_output = self.embedding(inputs).unsqueeze(1) # unqueeze = add a dim of size = 1 at indx = 1\n",
        "        layer_output, (hidden,cell) = self.rnn(layer_output,(hidden,cell))\n",
        "        layer_output = self.fc(layer_output).reshape(layer_output.size(0),-1) # output dim (dim0, mult_of_other_dims)\n",
        "        return layer_output, hidden, cell\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        hidden = torch.zeros(1,batch_size,self.rnn_hidden_size) # init hidden with all 0s - 1 is num layers in LSTM\n",
        "        cell = torch.zeros(1,batch_size,self.rnn_hidden_size) # init cell with all 0s\n",
        "        return hidden.to(DEVICE), cell.to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "d6afe4e0-8172-4f43-ac4a-757a0897c628",
      "metadata": {
        "id": "d6afe4e0-8172-4f43-ac4a-757a0897c628"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "vocab_size = len(word_array)\n",
        "embed_size = 256\n",
        "hidden_size = 512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "8eaf02bf-fd37-41c9-9e14-fc4fb0fdda6b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8eaf02bf-fd37-41c9-9e14-fc4fb0fdda6b",
        "outputId": "28919fb3-9cf8-44cb-c205-de607c897758"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatGeneratingRNN(\n",
              "  (embedding): Embedding(2813, 256)\n",
              "  (rnn): LSTM(256, 512, batch_first=True)\n",
              "  (fc): Linear(in_features=512, out_features=2813, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# creating model\n",
        "model = ChatGeneratingRNN(vocab_size, embed_size, hidden_size)\n",
        "model = model.to(DEVICE)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "2f5c34a5-5c97-470a-b8ff-e85e93ce7f75",
      "metadata": {
        "id": "2f5c34a5-5c97-470a-b8ff-e85e93ce7f75"
      },
      "outputs": [],
      "source": [
        "# setting output activation and loss\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "448f60cb-cfc8-4768-9af5-ab229211cd70",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "448f60cb-cfc8-4768-9af5-ab229211cd70",
        "outputId": "f8d9ef58-5a1f-435b-caa4-48c3ca95becb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [0/5001], Loss: 0.4349\n",
            "Epoch [500/5001], Loss: 0.3923\n",
            "Epoch [1000/5001], Loss: 0.3332\n",
            "Epoch [1500/5001], Loss: 0.3252\n",
            "Epoch [2000/5001], Loss: 0.2842\n",
            "Epoch [2500/5001], Loss: 0.2828\n",
            "Epoch [3000/5001], Loss: 0.2413\n",
            "Epoch [3500/5001], Loss: 0.2597\n",
            "Epoch [4000/5001], Loss: 0.2171\n",
            "Epoch [4500/5001], Loss: 0.2148\n",
            "Epoch [5000/5001], Loss: 0.2033\n"
          ]
        }
      ],
      "source": [
        "# training the model\n",
        "training_epochs = 5001\n",
        "model.to(DEVICE)\n",
        "model.train()\n",
        "for epoch in range(training_epochs):\n",
        "    hidden,cell = model.init_hidden(batch_size)\n",
        "    seq_batch, target_batch = next(iter(dataloader)) # get batches of batch size from the dataloader\n",
        "    seq_batch = seq_batch.to(DEVICE)\n",
        "    target_batch = target_batch.to(DEVICE)\n",
        "    optimizer.zero_grad()\n",
        "    loss = 0\n",
        "    for w in range(sequence_size): # w: 0 -> sequence_size - 1\n",
        "        pred, hidden, cell = model(seq_batch[:,w],hidden,cell) # run forward iteration, all rows entries at column = w\n",
        "        loss += loss_function(pred, target_batch[:,w]) # caculate cumulative loss, all rows entries at column w\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    loss = loss.item()/sequence_size # normalize loss\n",
        "    if epoch % 500 == 0:\n",
        "      print(f'Epoch [{epoch}/{training_epochs}], Loss: {loss:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "b393be8d-4d65-4166-b879-bdb76ed07bd5",
      "metadata": {
        "id": "b393be8d-4d65-4166-b879-bdb76ed07bd5"
      },
      "outputs": [],
      "source": [
        "def top_p_sampling(logits, temperature=1.0, top_p=0.9):\n",
        "    # Apply temperature scaling\n",
        "    scaled_logits = logits / temperature\n",
        "\n",
        "    # Convert logits to probabilities using softmax\n",
        "    probabilities = torch.softmax(scaled_logits, dim=-1)\n",
        "\n",
        "    # Sort probabilities and compute cumulative sum\n",
        "    sorted_indices = torch.argsort(probabilities, descending=True)\n",
        "    sorted_probabilities = probabilities[sorted_indices]\n",
        "    cumulative_probabilities = torch.cumsum(sorted_probabilities, dim=-1)\n",
        "\n",
        "    # Apply top-p filtering\n",
        "    indices_to_keep = cumulative_probabilities <= top_p\n",
        "    truncated_probabilities = sorted_probabilities[indices_to_keep]\n",
        "\n",
        "    # Rescale the probabilities\n",
        "    truncated_probabilities /= torch.sum(truncated_probabilities)\n",
        "\n",
        "    # Convert to numpy arrays for random choice\n",
        "    truncated_probabilities = truncated_probabilities.cpu().numpy()\n",
        "    sorted_indices = sorted_indices.cpu().numpy()\n",
        "    indices_to_keep = indices_to_keep.cpu().numpy()\n",
        "\n",
        "    # Sample from the truncated distribution\n",
        "    if not indices_to_keep.any():\n",
        "        # Handle the empty case - for example, using regular sampling without top-p\n",
        "        probabilities = torch.softmax(logits / temperature, dim=-1)\n",
        "        next_word_index = torch.multinomial(probabilities, 1).item() # sample 1 item based on probabilities\n",
        "    else:\n",
        "        # Existing sampling process\n",
        "        next_word_index = np.random.choice(sorted_indices[indices_to_keep], p=truncated_probabilities)\n",
        "\n",
        "    return torch.tensor(next_word_index).to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "dqL7YAYDAffR",
      "metadata": {
        "id": "dqL7YAYDAffR"
      },
      "outputs": [],
      "source": [
        "def generate(model, seed_string, len_generated_text=50, temperature=1.0, top_p=0.95):\n",
        "    seed_tokens = word_tokenize(seed_string.lower())\n",
        "    encoded_input = torch.tensor([word2int_mapping[t] for t in seed_tokens])\n",
        "    encoded_input = torch.reshape(encoded_input, (1, -1)).to(DEVICE) # reshape to 2d form (1, num_tokens)\n",
        "    generated_str = seed_string # init with input\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "        hidden, cell = model.init_hidden(1)\n",
        "        hidden = hidden.to(DEVICE)\n",
        "        cell = cell.to(DEVICE)\n",
        "        for w in range(len(seed_tokens) - 1):\n",
        "            _, hidden, cell = model(encoded_input[:, w].view(1), hidden, cell)\n",
        "        last_word = encoded_input[:, -1] # initializes with last word of seed string\n",
        "        for i in range(len_generated_text):\n",
        "            logits, hidden, cell = model(last_word.view(1), hidden, cell) # start generating starting with giving the last word as 1st input\n",
        "            logits = torch.squeeze(logits, 0)\n",
        "            last_word = top_p_sampling(logits.cpu(), temperature, top_p)  # Ensure logits is on CPU and replace last word with generated next word\n",
        "            generated_str += \" \" + str(word_array[last_word]) # add the generated next word tio generated_str\n",
        "\n",
        "    return generated_str.replace(\" . \", \". \") # ok thanks . i will look into it -> ok thanks. i will look into it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "5-FB9ur1A08b",
      "metadata": {
        "id": "5-FB9ur1A08b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c83cf2e-e430-40eb-eb48-c55a13e44190"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "how did that you enjoyed it ? human 1 : actually no idea of it was a little as well. do you have any plans for the break ? human 2 : what 's the book is about ? human 1 : it 's a sci-fi book about aliens yourself ? human\n"
          ]
        }
      ],
      "source": [
        "model.to(DEVICE)\n",
        "print(generate(model, seed_string='how did that'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "7b6e45f2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7b6e45f2",
        "outputId": "9fbd5103-5d42-4c7f-ba48-5570010ba51b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the human 1 : oh that 's cool ! what is your favorite ? human 2 : not great , but i know too much is things. any plans for the break ? human 1 : there 's the one of < redacted_term > used to run after death. i\n"
          ]
        }
      ],
      "source": [
        "print(generate(model, seed_string= 'the human'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "93e7f82e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93e7f82e",
        "outputId": "0bf0de3e-8ab2-4509-921e-6eed87fc186b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "once upon a time best ! human 2 : hello there ! who are you doing ? human 1 : i 've been reading the conference i prefer it everyday over the weekends. human 2 : lol i ca n't tell my cat the city and i think its best to start discussing\n"
          ]
        }
      ],
      "source": [
        "print(generate(model, seed_string='once upon a time'))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}