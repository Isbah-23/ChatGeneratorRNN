{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f558eb78-7683-46d5-ad22-38faa21d41c0",
      "metadata": {
        "id": "f558eb78-7683-46d5-ad22-38faa21d41c0"
      },
      "outputs": [],
      "source": [
        "# RNN which will (hopefully) generate a random chat between 2 people"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "c1a3f73c-0563-4923-9a57-1e2ee508425c",
      "metadata": {
        "id": "c1a3f73c-0563-4923-9a57-1e2ee508425c"
      },
      "outputs": [],
      "source": [
        "# get and unzip dataset - only need to run once\n",
        "# dataset = 'projjal1/human-conversation-training-data'\n",
        "# kaggle.api.dataset_download_files(dataset, path=\".\", unzip=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "cdedd7d8-34c6-4e67-b723-2b3401656923",
      "metadata": {
        "id": "cdedd7d8-34c6-4e67-b723-2b3401656923"
      },
      "outputs": [],
      "source": [
        "# # for kaggle\n",
        "# # import kaggle\n",
        "# for work\n",
        "import torch\n",
        "# import nltk\n",
        "# nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "62f16d83-ef5d-4a6a-8fa8-6bcb7a5c68c5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62f16d83-ef5d-4a6a-8fa8-6bcb7a5c68c5",
        "outputId": "41810303-c761-4ba4-ef87-dbfcd70a61f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['human', '1', ':', 'hi', '!', 'human', '2', ':', 'what', 'is', 'your', 'favorite', 'holiday', '?', 'human', '1', ':', 'one', 'where', 'i', 'get', 'to', 'meet', 'lots', 'of', 'different', 'people', '.', 'human', '2', ':', 'what', 'was', 'the', 'most', 'number', 'of', 'people', 'you', 'have', 'ever', 'met', 'during', 'a', 'holiday', '?', 'human', '1', ':', 'hard', 'to', 'keep', 'a', 'count', '.', 'maybe', '25.', 'human', '2', ':', 'which', 'holiday', 'was', 'that', '?', 'human', '1', ':', 'i', 'think', 'it', 'was', 'australia', 'human', '2', ':', 'do', 'you', 'still', 'talk', 'to', 'the', 'people', 'you', 'met', '?', 'human', '1', ':', 'not', 'really', '.', 'the', 'interactions', 'are', 'usually', 'short-lived', 'but', 'it', \"'s\"]\n",
            "Total words:  27943\n",
            "Unique words:  2813\n"
          ]
        }
      ],
      "source": [
        "# building vocabulary in token form\n",
        "# words = [line.lower().split() for line in lines] # using nltk tokens for better splitting (fine! vs fine)\n",
        "words = word_tokenize(text.lower())\n",
        "print(words[:100])\n",
        "print(\"Total words: \",len(words))\n",
        "print(\"Unique words: \", len(set((words))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "b8a1a826-fdde-42ae-84b3-71230c1105a0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8a1a826-fdde-42ae-84b3-71230c1105a0",
        "outputId": "b8c1a705-490b-4a94-e4b7-41217d7c6440"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total characters in text:  115782\n"
          ]
        }
      ],
      "source": [
        "# reading the data line by line into list\n",
        "text = None\n",
        "with open('human_chat.txt','r',encoding='utf-8') as dataset_file:\n",
        "    text = dataset_file.read()\n",
        "print(\"Total characters in text: \",len(text))\n",
        "# print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "535fbfdd-a016-4182-afae-6f05340f05a7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "535fbfdd-a016-4182-afae-6f05340f05a7",
        "outputId": "64971882-4c68-4c11-a40e-132f06b9bc62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2813\n"
          ]
        }
      ],
      "source": [
        "vocabulary = sorted(set(words))\n",
        "print(len(vocabulary))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "61436d04-433a-42d9-a5e5-4a10e65e9482",
      "metadata": {
        "id": "61436d04-433a-42d9-a5e5-4a10e65e9482"
      },
      "outputs": [],
      "source": [
        "# create lookup table for word to in and reverse\n",
        "word2int_mapping = {word:i for i,word in enumerate(vocabulary)}\n",
        "word_array = np.array(vocabulary)\n",
        "# reverse_vocabulary = {i+1: word for i, (word, _) in enumerate(word_counts.items())} # reverse as well\n",
        "# print(len(vocabulary) + 1)\n",
        "# print(vocabulary)\n",
        "# # print(reverse_vocabulary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "f5880daa-4023-4209-a48a-0064f440e256",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5880daa-4023-4209-a48a-0064f440e256",
        "outputId": "4fc00f48-bd19-4e17-d25f-5ae7227b3134"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1187   25   56 ... 1574 1281   20]\n",
            "Encoded lines shape: (27943,)\n",
            "Words ['human', '1', ':', 'hi', '!', 'human', '2', ':', 'what', 'is', 'your', 'favorite', 'holiday', '?', 'human', '1', ':', 'one', 'where', 'i']\n",
            "Encoding, [1187   25   56 1134    0 1187   33   56 2708 1278 2783  876 1159   60\n",
            " 1187   25   56 1664 2713 1203]\n",
            "Reverse conversion:  human 1 : hi ! human 2 : what is your favorite holiday ? human 1 : one where i\n"
          ]
        }
      ],
      "source": [
        "# encoding the sentences according to the vocabulary\n",
        "encoded_lines = np.array([word2int_mapping[word] for word in words], dtype=np.int32)\n",
        "print(encoded_lines)\n",
        "print(\"Encoded lines shape:\",encoded_lines.shape)\n",
        "print(\"Words\", words[:20])\n",
        "print(\"Encoding,\", encoded_lines[:20])\n",
        "print(\"Reverse conversion: \", ' '.join(word_array[encoded_lines[:20]]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "50c29fb7-cdc0-4cb8-8615-db08c8667aa2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50c29fb7-cdc0-4cb8-8615-db08c8667aa2",
        "outputId": "79914055-c1ad-4b50-9eb0-2210fdc162c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([1187,   25,   56, 1134,    0, 1187,   33,   56, 2708, 1278, 2783,\n",
            "        876, 1159,   60, 1187,   25,   56, 1664, 2713, 1203, 1009, 2501,\n",
            "       1491, 1427, 1642,  688, 1762,   20, 1187,   33,   56, 2708, 2681,\n",
            "       2461, 1556, 1628, 1642, 1762, 2781, 1103,  812, 1512,  749,   63,\n",
            "       1159,   60, 1187,   25,   56, 1095, 2501], dtype=int32)]\n",
            "\n",
            "[1187   25   56 1134    0 1187   33   56 2708 1278 2783  876 1159   60\n",
            " 1187   25   56 1664 2713 1203 1009 2501 1491 1427 1642  688 1762   20\n",
            " 1187   33   56 2708 2681 2461 1556 1628 1642 1762 2781 1103  812 1512\n",
            "  749   63 1159   60 1187   25   56 1095] -> 2501\n",
            "'human 1 : hi ! human 2 : what is your favorite holiday ? human 1 : one where i get to meet lots of different people . human 2 : what was the most number of people you have ever met during a holiday ? human 1 : hard' -> 'to'\n"
          ]
        }
      ],
      "source": [
        "# make sequences and chunks\n",
        "sequence_size = 50\n",
        "chunk_size = sequence_size + 1\n",
        "\n",
        "text_chunks = [encoded_lines[i:i+chunk_size] for i in range(len(encoded_lines)-chunk_size+1)]\n",
        "print(text_chunks[:1],end=\"\\n\\n\")\n",
        "\n",
        "for seq in text_chunks[:1]:\n",
        "    input_seq = seq[:sequence_size]\n",
        "    target = seq[sequence_size]\n",
        "    print(input_seq,'->',target)\n",
        "    print(repr(' '.join(word_array[input_seq])),'->',repr(''.join(word_array[target])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "62fb0a5e-d5b4-4b5b-835c-a33482f3ab6d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62fb0a5e-d5b4-4b5b-835c-a33482f3ab6d",
        "outputId": "94b4bda2-f87b-4032-eb07-684e4dc32c8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-a86c7261a9a2>:12: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:274.)\n",
            "  seq_dataset = TextDataset(torch.tensor((text_chunks)))\n"
          ]
        }
      ],
      "source": [
        "# convert into proper pytorch dataset form\n",
        "\n",
        "class TextDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, text_chunks):\n",
        "        self.text_chunks = text_chunks\n",
        "    def __len__(self):\n",
        "        return len(self.text_chunks)\n",
        "    def __getitem__(self,idx):\n",
        "        text_chunk = self.text_chunks[idx]\n",
        "        return text_chunk[:-1].long(), text_chunk[1:].long()\n",
        "\n",
        "seq_dataset = TextDataset(torch.tensor((text_chunks)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "aa8e8bed-1363-420f-b9a1-eb4a70dd13cb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aa8e8bed-1363-420f-b9a1-eb4a70dd13cb",
        "outputId": "960b10ef-5452-4f92-f078-9da8a8c903df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: 'human 1 : hi ! human 2 : what is your favorite holiday ? human 1 : one where i get to meet lots of different people . human 2 : what was the most number of people you have ever met during a holiday ? human 1 : hard'\n",
            "Target: '1 : hi ! human 2 : what is your favorite holiday ? human 1 : one where i get to meet lots of different people . human 2 : what was the most number of people you have ever met during a holiday ? human 1 : hard to'\n",
            "\n",
            "Input: '1 : hi ! human 2 : what is your favorite holiday ? human 1 : one where i get to meet lots of different people . human 2 : what was the most number of people you have ever met during a holiday ? human 1 : hard to'\n",
            "Target: ': hi ! human 2 : what is your favorite holiday ? human 1 : one where i get to meet lots of different people . human 2 : what was the most number of people you have ever met during a holiday ? human 1 : hard to keep'\n",
            "\n",
            "Input: ': hi ! human 2 : what is your favorite holiday ? human 1 : one where i get to meet lots of different people . human 2 : what was the most number of people you have ever met during a holiday ? human 1 : hard to keep'\n",
            "Target: 'hi ! human 2 : what is your favorite holiday ? human 1 : one where i get to meet lots of different people . human 2 : what was the most number of people you have ever met during a holiday ? human 1 : hard to keep a'\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# checking out our get item chunks functionality\n",
        "for i,(seq,target) in enumerate(seq_dataset):\n",
        "    print(\"Input:\",repr(' '.join(word_array[seq])))\n",
        "    print(\"Target:\",repr(' '.join(word_array[target])))\n",
        "    print()\n",
        "    if i == 2:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "69a4d405-b970-46a0-8d43-cec620fb5986",
      "metadata": {
        "id": "69a4d405-b970-46a0-8d43-cec620fb5986"
      },
      "outputs": [],
      "source": [
        "# create a dataloader\n",
        "batch_size = 32\n",
        "dataloader = torch.utils.data.DataLoader(seq_dataset,batch_size=batch_size,shuffle=True,drop_last=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "3f9e71bd-6a34-4d30-809b-9cbe6bc62b51",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3f9e71bd-6a34-4d30-809b-9cbe6bc62b51",
        "outputId": "e527413e-07f6-4e0b-a0bf-b8f6daba3606"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "# this code tells what to use, not neccessary if u dont have a gpu to run on\n",
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "34fc40fe-1b05-4960-8155-12e8271d42fa",
      "metadata": {
        "id": "34fc40fe-1b05-4960-8155-12e8271d42fa"
      },
      "outputs": [],
      "source": [
        "# my rnn\n",
        "class ChatGeneratingRNN(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.embedding = torch.nn.Embedding(vocab_size, embed_size)\n",
        "        self.rnn_hidden_size = hidden_size\n",
        "        self.rnn = torch.nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
        "        self.fc = torch.nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, inputs, hidden, cell):\n",
        "        layer_output = self.embedding(inputs).unsqueeze(1)\n",
        "        layer_output, (hidden,cell) = self.rnn(layer_output,(hidden,cell))\n",
        "        layer_output = self.fc(layer_output).reshape(layer_output.size(0),-1)\n",
        "        return layer_output, hidden, cell\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        hidden = torch.zeros(1,batch_size,self.rnn_hidden_size)\n",
        "        cell = torch.zeros(1,batch_size,self.rnn_hidden_size)\n",
        "        return hidden.to(DEVICE), cell.to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "d6afe4e0-8172-4f43-ac4a-757a0897c628",
      "metadata": {
        "id": "d6afe4e0-8172-4f43-ac4a-757a0897c628"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "vocab_size = len(word_array)\n",
        "embed_size = 256\n",
        "hidden_size = 512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "8eaf02bf-fd37-41c9-9e14-fc4fb0fdda6b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8eaf02bf-fd37-41c9-9e14-fc4fb0fdda6b",
        "outputId": "0a0d2924-ff87-4e94-b0d1-307260d781b6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatGeneratingRNN(\n",
              "  (embedding): Embedding(2813, 256)\n",
              "  (rnn): LSTM(256, 512, batch_first=True)\n",
              "  (fc): Linear(in_features=512, out_features=2813, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "# creating model, setting output activation and loss\n",
        "model = ChatGeneratingRNN(vocab_size, embed_size, hidden_size)\n",
        "model = model.to(DEVICE)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "2f5c34a5-5c97-470a-b8ff-e85e93ce7f75",
      "metadata": {
        "id": "2f5c34a5-5c97-470a-b8ff-e85e93ce7f75"
      },
      "outputs": [],
      "source": [
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "448f60cb-cfc8-4768-9af5-ab229211cd70",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "448f60cb-cfc8-4768-9af5-ab229211cd70",
        "outputId": "c4bb7dde-6b43-4b33-da10-157af0238c2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [0/5001], Loss: 0.1805\n",
            "Epoch [500/5001], Loss: 0.1706\n",
            "Epoch [1000/5001], Loss: 0.1361\n",
            "Epoch [1500/5001], Loss: 0.1916\n",
            "Epoch [2000/5001], Loss: 0.1714\n",
            "Epoch [2500/5001], Loss: 0.1692\n",
            "Epoch [3000/5001], Loss: 0.1872\n",
            "Epoch [3500/5001], Loss: 0.1980\n",
            "Epoch [4000/5001], Loss: 0.1551\n",
            "Epoch [4500/5001], Loss: 0.1749\n",
            "Epoch [5000/5001], Loss: 0.1878\n"
          ]
        }
      ],
      "source": [
        "# training the model\n",
        "training_epochs = 5001\n",
        "model.to(DEVICE)\n",
        "model.train()\n",
        "for epoch in range(training_epochs):\n",
        "    hidden,cell = model.init_hidden(batch_size)\n",
        "    seq_batch, target_batch = next(iter(dataloader))\n",
        "    seq_batch = seq_batch.to(DEVICE)\n",
        "    target_batch = target_batch.to(DEVICE)\n",
        "    optimizer.zero_grad()\n",
        "    loss = 0\n",
        "    for w in range(sequence_size):\n",
        "        pred, hidden, cell = model(seq_batch[:,w],hidden,cell)\n",
        "        loss += loss_function(pred, target_batch[:,w])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    loss = loss.item()/sequence_size\n",
        "    if epoch % 500 == 0:\n",
        "      print(f'Epoch [{epoch}/{training_epochs}], Loss: {loss:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "b393be8d-4d65-4166-b879-bdb76ed07bd5",
      "metadata": {
        "id": "b393be8d-4d65-4166-b879-bdb76ed07bd5"
      },
      "outputs": [],
      "source": [
        "def top_p_sampling(logits, temperature=1.0, top_p=0.9):\n",
        "    # Ensure logits are a PyTorch tensor and move to DEVICE\n",
        "\n",
        "    # Apply temperature scaling\n",
        "    scaled_logits = logits / temperature\n",
        "\n",
        "    # Convert logits to probabilities using softmax\n",
        "    probabilities = torch.softmax(scaled_logits, dim=-1)\n",
        "\n",
        "    # Sort probabilities and compute cumulative sum\n",
        "    sorted_indices = torch.argsort(probabilities, descending=True)\n",
        "    sorted_probabilities = probabilities[sorted_indices]\n",
        "    cumulative_probabilities = torch.cumsum(sorted_probabilities, dim=-1)\n",
        "\n",
        "    # Apply top-p filtering\n",
        "    indices_to_keep = cumulative_probabilities <= top_p\n",
        "    truncated_probabilities = sorted_probabilities[indices_to_keep]\n",
        "\n",
        "    # Rescale the probabilities\n",
        "    truncated_probabilities /= torch.sum(truncated_probabilities)\n",
        "\n",
        "    # Convert to numpy arrays for random choice\n",
        "    truncated_probabilities = truncated_probabilities.cpu().numpy()\n",
        "    sorted_indices = sorted_indices.cpu().numpy()\n",
        "    indices_to_keep = indices_to_keep.cpu().numpy()\n",
        "\n",
        "    # Sample from the truncated distribution\n",
        "    if not indices_to_keep.any():\n",
        "        # Handle the empty case - for example, using regular sampling without top-p\n",
        "        probabilities = torch.softmax(logits / temperature, dim=-1)\n",
        "        next_word_index = torch.multinomial(probabilities, 1).item()\n",
        "    else:\n",
        "        # Existing sampling process\n",
        "        next_word_index = np.random.choice(sorted_indices[indices_to_keep], p=truncated_probabilities)\n",
        "\n",
        "    return torch.tensor(next_word_index).to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(model, seed_string, len_generated_text=50, temperature=1.0, top_p=0.95):\n",
        "    seed_tokens = word_tokenize(seed_string.lower())\n",
        "    encoded_input = torch.tensor([word2int_mapping[t] for t in seed_tokens])\n",
        "    encoded_input = torch.reshape(encoded_input, (1, -1)).to(DEVICE)\n",
        "\n",
        "    generated_str = seed_string\n",
        "\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "        hidden, cell = model.init_hidden(1)\n",
        "        hidden = hidden.to(DEVICE)\n",
        "        cell = cell.to(DEVICE)\n",
        "\n",
        "        for w in range(len(seed_tokens) - 1):\n",
        "            _, hidden, cell = model(encoded_input[:, w].view(1), hidden, cell)\n",
        "\n",
        "        last_word = encoded_input[:, -1]\n",
        "        for i in range(len_generated_text):\n",
        "            logits, hidden, cell = model(last_word.view(1), hidden, cell)\n",
        "            logits = torch.squeeze(logits, 0)\n",
        "            last_word = top_p_sampling(logits.cpu(), temperature, top_p)  # Ensure logits is on CPU\n",
        "            generated_str += \" \" + str(word_array[last_word])\n",
        "\n",
        "    return generated_str.replace(\" . \", \". \")"
      ],
      "metadata": {
        "id": "dqL7YAYDAffR"
      },
      "id": "dqL7YAYDAffR",
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(DEVICE)\n",
        "print(generate(model, seed_string='Hello how'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-FB9ur1A08b",
        "outputId": "58c83400-c496-4f93-b540-7f1a5dfa27fb"
      },
      "id": "5-FB9ur1A08b",
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello how are you doing ? human 1 : i 'm great , thanks. i 'm getting ready for a skydiving lesson. human 2 : ooh , nice. that sounds adventurous. where is it ? human 1 : right near my home town : seville , spain .\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}